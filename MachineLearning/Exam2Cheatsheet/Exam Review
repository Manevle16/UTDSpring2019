1. Naiive Bayes
   Consider a patient that has recieved test results T+ meaning the test is positive for
   cancer. Calculate P(cancer|T+). Here is what we know about the probabilites of cancer
   and not cancer, and the probabilities of T+(test positive):

   P(cancer) = 0.008
   P(T+|cancer) = 0.98
   P(T+) = .005

   a) Write out the Baye's theorem using the notations above:
      P(cancer|T+) = (P(T+|cancer) X P(cancer))/P(T+)

   b) Write the mathematical expression to calculate P(cancer|T+) using your formula from
      a) above and the data above
      P(cancer|T+) =  (.98 X .008)/.005 = 1.568      

   c) Why is Naiive Bayes called "naiive"?
      Because the algorithm makes assumptions about the data that may or may not be correct. Naiive Bayes will 
      assume the presence or absence of a feature will not affect the presence or absence of another feature.

2. What is step 1 of the k-means algorithm?
   Assign every observation a random assignment to one of the clusters from 1 to k. 
   
3. What is a centroid in k-means clustering and how is it determined in the iterative cycles of
   the algorithm.
   The centroid is the mean of all the observations that fall into the clustter associated with it. Every iteration
   all the observations get put into the cluster corresponding with the closest centroid, then the new centroid for
   that cluster is recalculated. This will continue iterating until no new assignments to a cluster is made.
   
4. In one run of the KNN model, k=1 and in another run k=7.
   a) If the data set is very small and sparse, which k might be better? why?
      k = 1, because the data is sparse meaning observations which are far away from eachother will be considered
      for assigning the response if k=7 was chosen. Also there is not a lot of data to process, so a large k will
      result in the model considering a large portion of the observations to determine the response which will have a
      high chance in including observations from seperate classes making the model inaccurate in its predictions 
      introducing a lot of bias.
      
   b) if the data is very large, which k might be better? why?
      k = 7, because a a too small of a k will have a model with a very high variance since the category an 
      observation may be assigned to is highly dependent on only one observation. A larger k will allow it to 
      consider multiple observations, reducing any possible noise in the graph, allowing for a better model 
      to be developed for the large data set.
      
   c) Given knn with k=5, describe how knn:
      i) calculate the real value in regression
         Knn will select the 5 closest observations to the input observations and get the mean of all the training 
         responses associated with the 5 observations that were grabbed. This value will be the prediction the model 
         gives for the input observations.
         
      ii) selects a class in classification
          Knn will select the 5 closest observations to the input observations. Whichever category has the majority
          of responses for these 5 observations will be the prediction for the input observations.
5. 
  a) In k-means clustering, k centroids are:
     The mean for each cluster k's observations.
     
  b) In the k-means algorithm, data points are assigned to a cluster in what fashion?
     Whichever centroid is closest to the data point is assigned to the cluster associated with the centroid.
     
  c) If your algorithm performs well on the training data but poorly on test data, you are 
     suffering from? 
     Overfitting.
     
  d) In linear regression, what are residuals?
     The difference between the prediciton and the actual response.
     
  e) * In Logistic regression model, which variables were important and which ones were not?
  f) * What was the accuracy of the logistic regression model and decision tree model?
  g) Is it a good idea to prune the tree? Why or why not?
     Its only a good idea to prune the tree if it is having overfitting issue, or if a better understanding
     of the observations effect on the response is needed.
     
  h) * Compare the accuracy of the logistic regression and the decision tree?
  i) * Which model (LR, or DT) might be more meaningful to a doctor, and why?
  j) * Which variables were important (2 or 3 **) in the LR model?
  k) * Which variables were used to created the DT?
  l) * What was the accuracy of the pruned tree?

6)
  a) Is KNN a parametric algorithm?
     Nonparametric since there is no set number of parameters for the model.
     
  b) can KNN be used for both classification and regression?
  Yes.
  
  c) P(A|B) = P(B|A) X P(A) / P(B)
     Need to know what each component is?
     P(A|B) - The probability of B given A occurs
     P(B|A) - The probability of A given B occurs
     P(A) - The probability of A occuring
     P(B) - The probability of B occuring
     
  d) Does decision tree tend to be less accurate than other supervised methods?
     Yes.
     
  e) Decision trees can be used for both classification and regression? T/F
     T
     
  f) Decision trees are always more accurate after pruning? T/F
     F
     
  g) What is characterized by low bias and high variance? (LR, LOGR, DT)
     Decision Trees
     
  h) What is characterized by high bias, low variance?
  Linear and Logistic Regression
  
  i) Why do we prune decision trees?
     Sometimes the tree may be overfitting the problem, or a better understanding of the predictors effect
     on the outcome is desired.
     
  j) In which type of learning target variable is a real number? and a class variable?
     Real number - Quantitative
     Class Variable - Qualitative
     
  k) What is the prupose of train and test data sets?
     The training data set allows the model to have something to base its predictions on and be built with. 
     The test data set allows the model to be tested for how well it works on unbefore seen data.
  
7) In a given problem, we are interested in using the ID3 decision tree induction algorithm to
   automatically determine something. Show the decision tree created by the ID3 desion tree learning algorithm. Show     the information gain calculations that you computed to create the tree.
   It would be take unrealiztically long to compute all the information gains in respects to AHD in respect to the
   root since a portion of the attribute of the heart.csv are numerical and would result in way too many factors to
   calculate. So I will not be calculating the root, rather I will show a calculation for a couple entropies, and one
   information gain since that should prove I know how to do the calculations. The highest info gain, if I had done
   them all, would have been chosen as the root.
   
   Entropy(AHD) = -(137/297)log2(137/297) - (160/297)log2(160/297) = .996
   Entropy(Thal|fixed) = -(12/18)log2(12/18) - (6/18)log2(6/18) = .918
   Entropy(Thal|normal) = -(127/164)log2(127/164) - (37/164)log2(37/164) = .77
   Entropy(Thal|reverseable) = -(27/115)log2(27/115) - (88/115)log2(88/115) = .786
   Gain(AHD|Thal) = .996 - ((18/297) * .918) - ((164/297) * .77) - ((115/297) * .786) = .211
   
   For an example of a tree on heart, look at HeartTree.R file

* ISLR library and attach the default data set; and also you can look up Heart data set
Programming Hints:
    i) Download the heart data to your machine
   ii) Load the data into R and attach it
  iii) Remove the "X" column
   iv) Set up train and test sets with 80% training using a seed
    v) Create LR model on training where AHD is predicted by all other variables
   vi) Run a summary of the model
  vii) Predict Yes/No on the test data
 viii) Compute the accuracy

    i) Create a decision tree model on the training data
   ii) Run a summary of the model
  iii) Make predictions of the test data
   iv) Compute the accuracy
    v) Print the tree with labels
   vi) Cross Validation
       a) Create a new tree from the cv.tree() function
       b) Look at the $dev and $size variables by the displaying the tree using its name
  vii) Prune the tree
       a) create a new pruned tree using best=n where n is the optimal size indicated in step vi) above
       b) plot the new pruned tree with labels
 viii) Predict
       a) Using the pruned tree, make predictions on the test data set
       b) Compute the accuracy